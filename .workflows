# BOR Workflow Service - Workflow Documentation

## file_ingestion.py ################################################################################## start


### Overview
The File Ingestion Workflow manages the process of ingesting files into the BOR system. It handles file transfer from the bor-files container to 
the bor-db container using the 'LOAD DATA INFILE' command, the data is then processed by a stored procedure.

### Business Case
- External systems upload files to the bor-files container via FTP
- The workflow service monitors for new files
- Files are processed and loaded into the bormeta database which serves as a staging area for further validation and processing on the way to the 
final table(s) in databases such as borinst. 
- Data is made available for further processing

### Technical Components

#### 1. Tasks
- `check_file_in_volume`: Verifies file existence in shared volume's incoming/ directory
  - Input: file_path (str) relative to root of the volume
  - Output: bool
  - Caching: Enabled (1 hour expiration)

- `load_file_to_borarch`: Loads file into borarch.FundClassFee using 'LOAD DATA INFILE' command using mounted volume shared with the bor-files and bor-deb containers
  - Input: file_path, db_host, db_port, db_user, db_pass, db_name
  - Output: bool
  - Retries: 3 attempts, 60-second delay

- `move_file_to_processed`: Moves file using direct volume access to the bor-files container processed/ directory from the incoming/ directory
  - Input: file_path, ftp_user, ftp_pass
  - Output: bool
  - Retries: 3 attempts, 60-second delay

- `execute_stored_procedure`: Runs database procedure
  - Input: proc_name, db_host, db_port, db_user, db_pass, db_name
  - Output: bool
  - Retries: 3 attempts, 60-second delay

#### 2. Flow
- Name: "File Ingestion Workflow"
- Entry Point: file_ingestion_workflow()
- Parameters:
  - file_path (str): Path to the file relative to the root of the volume in the bor-files container
  - ftp_host (str, default="bor-files"): FTP server hostname
  - ftp_port (int, default=21): FTP server port
  - ftp_user (str, optional): FTP username
  - ftp_pass (str, optional): FTP password
  - db_host (str, default="bor-db"): Database hostname
  - db_port (int, default=3306): Database port
  - db_user (str, optional): Database username
  - db_pass (str, optional): Database password
  - db_name (str, default="bormeta"): Database name
  - proc_name (str, default="usp_FileClassFee_Load"): Stored procedure name
  
#### 3.a. Dependencies external
- Python 3.11+
- Prefect
- mysql-connector-python
- python-dotenv

#### 3.b. Dependencies workflows
n/a

#### 4. Environment Variables
Required:
- FTP_ETL_USER: FTP username
- FTP_ETL_PASS: FTP password
- DB_ALL_SVC_USER: Database username
- DB_ALL_SVC_USER_PASSWORD: Database password

#### 5. File System Requirements
- Access to bor-files-data volume
- Mount point: /home/vsftpd
- Directory structure:
  ```
  /home/vsftpd/<ftp_user>/
  ├── incoming/    # New files
  ├── processed/   # Processed files
  └── archive/     # Archived files
  ```

#### 6. Database Requirements
- MySQL 8.0+
- Stored procedure: bormeta.usp_FundClass_Load
- User permissions: FILE privilege for bulk loading

### Implementation Steps
1. Create workflow file (src/workflows/file_ingestion.py)
2. Implement tasks with proper error handling and retries
3. Create main flow with parameter validation
4. Add environment variable support
5. Implement both FTP and direct access methods
6. Add comprehensive testing

### Testing
Location: tests/test_file_ingestion.py
Test Cases:
1. File not found scenario
2. DB connection failure scenario
3. File load failure scenario
4. File move failure scenario
5. Stored procedure failure scenario

### Error Handling
- File existence verification
- FTP connection retries
- Database connection retries
- File move operations
- Stored procedure execution

### Monitoring
- Task execution status
- File transfer success/failure
- Database procedure execution
- Error logging

### Security Considerations
- Non-root user execution
- Secure credential handling
- File permission management
- Database access control

### Performance Considerations
- Caching for file checks
- Retry mechanisms for network operations
- Direct file access optimization
- Database connection pooling

### Maintenance
- Regular log review
- Error pattern monitoring
- Performance metrics tracking
- Security updates

### Future Enhancements
1. File validation before processing
2. Archive management
3. Notification system
4. Performance optimization
5. Enhanced error reporting

## file_ingestion.py ################################################################################## end

## mellon_holdings_etl.py ################################################################################## start

### Overview
The Mellon Holdings ETL Workflow processes Mellon holdings CSV files and loads them into the BOR system databases. It handles file loading, data transformation, and account-specific data cleanup to ensure data integrity across multiple file imports. The workflow uses a two-step process: loading raw CSV data into MellonRawStaging, then transforming and inserting into MellonHoldings.

### Business Case
- Mellon provides holdings data in CSV format for multiple accounts
- Files are uploaded to the bor-files container via FTP
- The workflow processes each file using a two-step approach:
  1. Load raw CSV data into borarch.MellonRawStaging (all VARCHAR fields)
  2. Transform and insert data into borarch.MellonHoldings (typed fields)
- Account-specific cleanup ensures no duplicate or stale data for the same account and date
- Data is made available for further processing and analysis

### Technical Components

#### 1. Tasks
- `delete_existing_account_data`: Deletes existing records for accounts and dates found in the incoming file
  - Input: file_path (str), db_config (dict)
  - Output: bool
  - Purpose: Ensures clean data by removing old records for the same account and date combination

- `update_file_tracking`: Updates the MellonFileImport tracking table
  - Input: file_source (str), db_config (dict)
  - Output: bool
  - Purpose: Records file import attempts and status

- `load_data_to_staging`: Inherited from BaseIngestionWorkflow, loads CSV data using 'LOAD DATA INFILE'
  - Input: file_path, database connection parameters, field mappings, transformations
  - Output: bool
  - Retries: 3 attempts, 60-second delay
  - Purpose: Loads raw CSV data into MellonRawStaging (all VARCHAR fields)

- `update_filesource_for_loaded_data`: Updates FileSource column in MellonRawStaging
  - Input: file_source (str), db_config (dict)
  - Output: bool
  - Purpose: Sets FileSource for all rows loaded from a specific file

- `transform_loaded_data`: Transforms data from MellonRawStaging to MellonHoldings
  - Input: file_source (str), db_config (dict)
  - Output: bool
  - Purpose: Performs data type conversions and cleaning via INSERT...SELECT

- `run_mellon_integration_proc`: Runs the Mellon holdings integration stored procedure
  - Input: db_config (dict)
  - Output: bool
  - Purpose: Executes usp_mellon_hold_integrate(1, 'FULL_INTEGRATION', NULL, NULL, NULL, NULL) in bormeta database
  - Database: Connects to bormeta database (not borarch)
  - Features: Handles stored procedure results, error handling, connection cleanup

#### 2. Flow
- Name: "mellon-holdings-etl"
- Entry Point: mellon_holdings_etl_flow()
- Parameters:
  - source_files (list): List of source file paths (already in container)
  - db_host (str, default="bor-db"): Database hostname
  - db_port (str, default="3306"): Database port
  - db_user (str): Database username
  - db_password (str): Database password
  - db_name (str, default="borarch"): Database name
  - target_dir (str, default="/var/lib/mysql-files/ftpetl/incoming/"): Target directory
  - delimiter (str, default=","): Field delimiter
  - quote_char (str, default='"'"): Quote character
  - line_terminator (str, default="\n"): Line terminator
  - skip_lines (int, default=1): Number of header lines to skip
  - truncate_before_load (bool, default=False): Whether to truncate table before loading

#### 3.a. Dependencies external
- Python 3.11+
- Prefect
- mysql-connector-python
- python-dotenv
- csv (standard library)

#### 3.b. Dependencies workflows
- BaseIngestionWorkflow (src/utils/base_ingestion.py)

#### 4. Environment Variables
Required:
- DB_HOST: Database hostname
- DB_PORT: Database port
- DB_USER: Database username
- DB_PASSWORD: Database password
- DB_NAME: Database name

#### 5. File System Requirements
- Access to bor-files-data volume
- Mount point: /var/lib/mysql-files/ftpetl/incoming/
- Directory structure:
  ```
  /var/lib/mysql-files/ftpetl/incoming/
  ├── mellon-*.csv    # Mellon holdings files
  ```

#### 6. Database Requirements
- MySQL 8.0+
- Databases: borarch, bormeta

**Tables:**
- `borarch.MellonRawStaging`: Intermediate staging table for raw CSV data
  - Primary key: id (AUTO_INCREMENT)
  - All fields: VARCHAR(255) - stores raw CSV data before transformation
  - Key fields: AccountNumber, MellonSecurityId, AsOfDate, FileSource
  - Purpose: Holds raw CSV data before type conversion and cleaning

- `borarch.MellonHoldings`: Final processed data table
  - Primary key: id (AUTO_INCREMENT)
  - Key fields: AccountNumber, MellonSecurityId, AsOfDate, FileSource
  - Data types: VARCHAR(20-200), DECIMAL(10-20,2-6), DATE, DATETIME, INT
  - Indexes: idx_account (AccountNumber), idx_security (MellonSecurityId), idx_date (AsOfDate), idx_processed (Processed), idx_file_source (FileSource)
  - Purpose: Final destination for processed holdings data

- `borarch.MellonFileImport`: File import tracking table
  - Primary key: id (AUTO_INCREMENT)
  - Unique key: FileName
  - Fields: FileName, FileSize, ImportDate, RecordsImported, RecordsProcessed, Status, ErrorMessage, ProcessingDate
  - Status enum: 'IMPORTED', 'PROCESSED', 'ERROR'
  - Indexes: idx_filename (FileName), idx_status (Status), idx_import_date (ImportDate)

**Stored Procedures:**
- `borarch.usp_MellonHoldings_Load`: Loads Mellon holdings CSV file into staging table
  - Parameters: p_FilePath (VARCHAR(255)), p_FileSource (VARCHAR(100))
  - Purpose: Alternative method for loading data using LOAD DATA INFILE
  - Usage: CALL usp_MellonHoldings_Load('incoming/mellon-file.csv', 'mellon-file.csv')
  - Features: File tracking, data transformation, error handling

- `borarch.usp_MellonRawHoldings_Load`: Loads raw CSV data for debugging
  - Parameters: p_FilePath (VARCHAR(255))
  - Purpose: Loads raw CSV lines into MellonRawStaging table
  - Usage: CALL usp_MellonRawHoldings_Load('ftpetl/incoming/mellon-file.csv')

- `bormeta.usp_mellon_hold_integrate`: Integration procedure for Mellon holdings data
  - Parameters: p_AccountId (INT), p_IntegrationType (VARCHAR(50)), p_StartDate (DATE), p_EndDate (DATE), p_AccountNumber (VARCHAR(50)), p_SecurityId (VARCHAR(50))
  - Purpose: Integrates Mellon holdings data with other system data
  - Usage: CALL usp_mellon_hold_integrate(1, 'FULL_INTEGRATION', NULL, NULL, NULL, NULL)
  - Features: Data integration, validation, and processing

**User permissions required:**
- FILE privilege for bulk loading
- DELETE privilege for account cleanup
- INSERT/UPDATE privileges on all Mellon tables
- EXECUTE privilege on stored procedures
- Access to both borarch and bormeta databases

### Implementation Steps
1. Create workflow file (src/workflows/mellon_holdings_etl.py)
2. Implement account-specific deletion task
3. Implement file tracking task
4. Create main flow with parameter validation
5. Add field mappings and transformations for data cleaning
6. Configure deployment in prefect.yaml
7. Add comprehensive testing

### Testing
Location: tests/data/ (sample files)
Test Cases:
1. Single file processing
2. Multiple file processing
3. Account-specific data cleanup
4. Data transformation validation
5. File tracking verification
6. Integration procedure execution

### Error Handling
- File existence verification
- Database connection retries
- Account-specific deletion errors
- Data transformation errors
- File tracking errors
- Integration procedure execution errors

### Monitoring
- Task execution status
- File processing success/failure
- Account cleanup operations
- Data transformation results
- Integration procedure execution results
- Error logging

### Security Considerations
- Non-root user execution
- Secure credential handling
- Database access control
- File permission management

### Performance Considerations
- Account-specific deletion optimization
- Data transformation efficiency
- Database connection pooling
- Batch processing for multiple files

### Maintenance
- Regular log review
- Error pattern monitoring
- Performance metrics tracking
- Data quality validation

### Future Enhancements
1. Data validation before processing
2. Archive management
3. Notification system
4. Performance optimization
5. Enhanced error reporting
6. Data quality metrics

## mellon_holdings_etl.py ################################################################################## end