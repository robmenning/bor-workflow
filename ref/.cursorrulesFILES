::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

This project will create a file storage container for use with the bor project, most specifically the bor-workflow and bor-db containers. 
External applications and processes will be able to move files to/from this file storage container using sftp or other modern transfer protocols. 
The storage container will be accessible from outside its docker container by sftp and over the docker network or azure network (depending on the deployment environemnt) by the bor-* containers over a private network on the host. 
A top requirement is that this is a light and simple to admin implementation of an sftp server, file organization and file storage container.
The context is an early stage project that has a next.js web application, a node express api, a mysql 8 database, a workflow/et. service. All are running in separate processes in separate docker containers. 

This and the other bor-* services will be deployable as docker containers in the following production environments:
p1. linux host with docker running the docker containers, docker network, shared and non shared persistent volumes. 
p2. azure host with AKS using the Azure container registry, ??? for the private inter-service network, and ??? for data that can be shared. 

On the production environments, two instances of this (and all bor-* services) can be running (on different ports as specified below) for development, staging and production. Specifics will be handled by .env files. 

This service will be used initially in the following business case: a file is sftpd by an external system into this service's storage container; the bor-workflow container/service will execute a mysql command in the bor-db mysql container/service which will use 'load data infile...' to import the file into a database table, meaning that the bor-db container will need appropriate access to the files received and stored by this service. 
In the docker on linux bare metal host deployment environment, a docker network is used for inter-service communication, and the bor-files (this) service/container, the bor-db, and bor-workflow mount the same persistent volumes as a way of sharing data. It may have to be a bit different for azure deployments. 

A simple and flexible implementation is required.

The service will run in peroduction in a docker container called bor-files
System port scheme:
container      dev   stage prod  docker-internal
-----------------------------------------------
bor-app        4400  4500  4600  3000
bor-api        4410  4510  4610  4000
bor-db         4420  4520  4620  3306
bof-message    4430  4530  4630  9092
bor-workflow   4440  4540  4640  8080
bor-etl        4450  4550  4650  8888
bor-svc-calc   4460  4560  4660  5000
bor-files      4470  4570  4670    22 * this container/service

Technology, general:
- This project will create an sftp server and file storage container to meet the requirements and deployment environments. 
- A docker network called 'bor-network' is used for interserice communication between this and other docker containers in the system.
- the project will use modern, best practices, and industry standard technologies for web application architecture and development.
- The service will mount a persistent volume to store the files which can also be mounted by other bor-* services connected to the bor-network or other private inter-container network.
- Configure network access for both external SFTP and internal container communication
- Use emberstack/sftp server in a Docker container, which implements a small and flexible docker image with ftps support

User management features:
   - Add/modify users with default inbound, outbound, archive directories
   - Add/modify user access 

Data Storage:
- ...

Volume Information:
- Docker Volumes:
  1. bor-files-data
     - Purpose: Stores all SFTP user data and files
     - Mount point: /home/sftp
     - Permissions: 
       * Root directories: 755 (drwxr-xr-x)
       * User directories: 770 (drwxrwx---)
       * Files: 660 (-rw-rw----)
     - Owner: ftp
     - Group: mysql
     - Structure:
       /home/sftp/
       └── users/
           └── <username>/
               ├── inbound/    # For incoming files (used by bor-workflow and bor-db)
               ├── outbound/   # For outgoing files
               └── archive/    # For archived files
     - Compatibility Requirements:
       * All containers (bor-files, bor-workflow, bor-db) mount at /home/sftp
       * MySQL LOAD DATA INFILE operations use /home/sftp/users/<username>/inbound/
       * File permissions must allow mysql group access for LOAD DATA INFILE
     - Reference for bor-workflow:
       * Mount volume at: /home/sftp
       * Access files at: /home/sftp/users/<username>/inbound/
       * Ensure mysql group membership for file access
       * Update workflow paths to use this structure
     - Reference for bor-db:
       * Mount volume at: /home/sftp
       * LOAD DATA INFILE path: /home/sftp/users/<username>/inbound/
       * Ensure mysql group membership for file access
       * Update MySQL configuration to use this path

  2. bor-files-host-key
     - Purpose: Stores SSH host keys for SFTP server
     - Mount point: /etc/ssh
     - Contains: RSA, ECDSA, and ED25519 host keys
     - Persists across container restarts

Volume Mounting/access in Other Containers:
1. Docker Network Environment (Linux Host):
   - All bor-* containers are connected via 'bor-network' Docker network
   - bor-files-data volume is mounted in:
     * bor-files container (primary) at /home/sftp
     * bor-db container at /home/sftp
     * bor-workflow container at /home/sftp
   - Volume access is controlled by:
     * File permissions (770 for directories, 660 for files)
     * Group membership (mysql group)
     * Container network isolation
   - Directory Structure:
     * All containers use the same path structure
     * SFTP users upload to: /home/sftp/users/<username>/inbound/
     * bor-workflow and bor-db read from: /home/sftp/users/<username>/inbound/
     * No file movement required between directories
   - Implementation Requirements for bor-workflow:
     * Update Dockerfile to mount volume at /home/sftp
     * Update workflow scripts to use /home/sftp/users/<username>/inbound/
     * Ensure container has mysql group membership
     * Update any hardcoded paths in workflow definitions
   - Implementation Requirements for bor-db:
     * Update Dockerfile to mount volume at /home/sftp
     * Update MySQL configuration for LOAD DATA INFILE
     * Ensure container has mysql group membership
     * Update any stored procedures using file paths

2. Azure Environment (AKS):
   - Volume access details to be determined based on Azure storage options
   - Considerations:
     * Azure Files for persistent storage
     * Azure Disk for high-performance requirements
     * Network security groups for inter-service communication
     * Azure Private Link for secure service-to-service communication
   - Directory structure must be maintained across all environments
   - Same mount points and permissions apply in Azure environment

3. Volume Management:
   - Volumes are created during container initialization
   - Volume cleanup is handled in build-and-run.sh
   - Volume health is monitored via monitor.sh
   - Volume usage can be checked using:
     * docker volume inspect bor-files-data
     * docker exec bor-files du -h /home/sftp
     * ./scripts/monitor.sh disk
   - Verification Steps for bor-workflow and bor-db:
     * Check volume mount point is /home/sftp
     * Verify mysql group membership
     * Test file access permissions
     * Confirm workflow/database operations can access files

4. Volume Security:
   - Host keys are stored in separate volume (bor-files-host-key)
   - User data is isolated in chrooted directories
   - File permissions enforce least privilege
   - Group-based access control for inter-container communication
   - Security Requirements for bor-workflow and bor-db:
     * Must use mysql group for file access
     * Must maintain file permissions (660)
     * Must not modify directory permissions (770)
     * Must not change file ownership

User Directory Structure:
/home/<username>/
├── inbound/    
├── outbound/   
└── archive/    

Permissions:
- Directories: 770 (drwxrwx---)
- Files: 660 (-rw-rw----)
- Owner: ftp
- Group: mysql

## technical details 
This project will be developed to a high standard of quality but will favour simplicity over sophistication in design and implementation.
Advice and input from AI will be given as an expert professional using industry best practices for the technologies used. 
All secrets will be stored in .env files using standard techniques for the technologies used. .env files will follow this naming convention:
- .env
- .env.local

## technologies used
- Typescript
- emberstack/sftp in a Docker container
- Docker without docker compose
- Git
- Github and github actions for CI/CD to production

## developer details
You are a senior, experiences full stack system architect, designer and developer with years of production code in the above listed technologies. 
You have experience in data-centric environments including data management, etl, data validation, data processing. 
You follow industry best practices. 
You take pride in the quality of your work. 
You are technically competent but prefer simple implementations. 
You carefully consider tradeoffs to be made during design decisions. 

#### Usage
# sftp a file from a local machine to the bor-file container on the same machine

# sftp a file from a local machine to the bor-file container on a different machine

# use a file in this bor-file container by another container (e.g. bor-db) over the docker network (not FTP)

### Monitoring

The monitoring system is implemented through `monitor.sh` which provides comprehensive monitoring capabilities for the FTP server:

# testing requires openssh-client:
# sudo apt-get update && sudo apt-get install -y openssh-client

#### Available Commands
- `status` - Shows container health, resource usage, and running status
- `connections` - Displays active FTP connections, listening ports, and server process status
- `disk` - Reports volume usage, directory sizes, and storage metrics
- `logs` - Shows recent FTP server logs
- `users` - Displays user permissions and quota information
- `all` - Runs all monitoring checks

#### Key Features
- Container health monitoring
- Resource usage tracking
- Active connection monitoring
- Disk space and volume management
- Log file inspection
- User permission verification
- Quota monitoring

## User Management

The user management system is implemented through `manage-users.sh` which provides comprehensive user administration capabilities:

#### Available Commands
- `list` - Lists all FTP users and their directories
- `add username password` - Creates a new FTP user with specified credentials
- `delete username` - Removes an FTP user and their directories
- `modify username [options]` - Updates user settings
- `info username` - Displays detailed user information

#### Key Features
- Username validation (3-32 chars, alphanumeric with underscores/hyphens)
- Password validation (minimum 8 characters)
- Automatic directory structure creation (inbound/outbound/archive)
- Permission management (default 755)
- Quota management
- User information display including:
  - Directory structure
  - Permissions
  - Quota status
  - Login history


## Testing
tests are in ./scripts/test.sh

#### Test Categories
- Basic Connectivity (sFTP server reachability)
- Authentication (login functionality)
- File Operations (upload/download)
- Directory Operations (create/list)
- Permission Verification
- Passive Mode Support
- Large File Handling
- Concurrent Transfer Testing
- Container Network Access
- File Processing Workflow
- Archive Functionality
- Quota Management

#### Key Features
- Environment-aware testing (development/production)
- Automated test execution
- Comprehensive test coverage
- Detailed test reporting
- Automatic cleanup
- Container health verification
- Cross-container communication testing

#### Test Requirements
- Docker
- curl
- netcat (nc)
- ftp client

#### Test Environment
- Supports multiple environments (dev/stage/prod)
- Configurable ports per environment
- Environment-specific credentials
- Automatic test file generation
- Clean test environment management

## RBAC considerations
tbd

## deployment 
The project will be deployed using standard Docker techniques, but not using Docker Compose which is not supported in some target environments.
The project will be deployed using a CI/CD pipeline to a target environment.
 # to ensure prod has all of the .env files, some of which are .gitignored 
scp bor-files/.env* robmenning.com@xenodochial-turing.108-175-7-118.plesk.page:/var/www/vhosts/robmenning.com/bor/bor-files/

# check and open PORTS on prod (docker) to make ./scripts/test-prod-external.sh run 
$ nc -vz xenodochial-turing.108-175-7-118.plesk.page 4670 #quick port test;
nc: connect to xenodochial-turing.108-175-7-118.plesk.page (108.175.7.118) port 4670 (tcp) failed: Connection timed out
(port not open)
$ sudo ufw status
Status: inactive

$ sudo iptables -L -n
<lots of output>

$ 

$ 


# DEV ITERATIONS
DEV:
1. clear; docker stop bor-files; docker rm bor-files; ./scripts/build-and-run.sh && echo "sleep 4"; sleep 4; docker ps; docker logs bor-files #bor-files
2. push to git

PROD:
3. ssh with robmenning.com account to /var/www/vhosts/robmenning.com/bor/bor-files/ and 
$ git pull origin <dev?>
4. ssh with root account to /var/www/vhosts/robmenning.com/bor/bor-files/ and 
$ clear; docker stop bor-files; docker rm bor-files; ./scripts/build-and-run.sh production && echo "sleep 4"; sleep 4; docker ps; docker logs bor-files #bor-files 

## running and monitoring in different environments:

